{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import KFold, cross_validate\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Using Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and load the data using Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML100K\n",
      "    userID  itemID  rating  timestamp\n",
      "0     196     242       3  881250949\n",
      "1     186     302       3  891717742\n",
      "2      22     377       1  878887116\n",
      "3     244      51       2  880606923\n",
      "4     166     346       1  886397596\n",
      "\n",
      "\n",
      "\n",
      "PDA2018\n",
      "    userID  itemID  rating  timeStamp\n",
      "0       5     648       5  978297876\n",
      "1       5    1394       5  978298237\n",
      "2       5    3534       5  978297149\n",
      "3       5     104       4  978298558\n",
      "4       5    2735       5  978297919\n",
      "Shapes before sub-sampling:\n",
      "(100000, 4)\n",
      "(470711, 4)\n",
      "\n",
      "Shapes after sub-sampling:\n",
      "(97623, 4)\n",
      "(465154, 4)\n",
      "\n",
      " Some general information on the training sets we will be using:\n",
      "1) Number of items in each dataset  ML100k: 1682 PDA: 1824\n",
      "2) Number of users in each dataset  ML100k: 943 PDA: 5690\n",
      "3) Number of ratings in each dataset  ML100k: 100000 PDA: 470711\n",
      "4) Mean rating  ML100k: 3.52986 PDA: 3.638361967321775\n"
     ]
    }
   ],
   "source": [
    "# Read both the datasets from the files using pandas\n",
    "movielens_df = pd.read_csv(\"../data/u.data\", sep=\"\\t\", header=None)\n",
    "movielens_df.columns = [\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
    "pda_df = pd.read_csv(\"../data/train-PDA2018.csv\", sep=\",\")\n",
    "print(\"ML100K\\n\", movielens_df.head())\n",
    "print(\"\\n\\n\")\n",
    "print(\"PDA2018\\n\", pda_df.head())\n",
    "\n",
    "# Sample the data such that every user has rated at least 10 items and every item has been by at least 10 users\n",
    "print(\"Shapes before sub-sampling:\")\n",
    "print(movielens_df.shape)\n",
    "print(pda_df.shape)\n",
    "\n",
    "# Movielens users all have at least 20 ratings so no need to subsample the user values\n",
    "ml_subsampled = movielens_df[movielens_df['itemID'].isin(movielens_df['itemID'].value_counts()[movielens_df['itemID'].value_counts()>10].index)]\n",
    "pda_subsampled = pda_df[pda_df['itemID'].isin(pda_df['itemID'].value_counts()[pda_df['itemID'].value_counts()>10].index)]\n",
    "pda_subsampled = pda_subsampled[pda_subsampled['userID'].isin(pda_subsampled['userID'].value_counts()[pda_subsampled['userID'].value_counts()>10].index)]\n",
    "print(\"\\nShapes after sub-sampling:\")\n",
    "print(ml_subsampled.shape)\n",
    "print(pda_subsampled.shape)\n",
    "\n",
    "# Create the training datasets using Surprise's reader class\n",
    "reader = Reader(rating_scale=(1,5)) # We have ratings from 1 to 5 so we create the rating scale\n",
    "\n",
    "# Load the data from the dataframes\n",
    "movielens_dataset = Dataset.load_from_df(movielens_df.iloc[:,0:3], reader)\n",
    "pda_dataset = Dataset.load_from_df(pda_df.iloc[:,0:3], reader)\n",
    "\n",
    "# Build full trainsets to print out the data loaded above\n",
    "mls_train = movielens_dataset.build_full_trainset()\n",
    "pda_train = pda_dataset.build_full_trainset()\n",
    "\n",
    "# Print out some basic information about the datasets\n",
    "print(\"\\n Some general information on the training sets we will be using:\")\n",
    "print(\"1) Number of items in each dataset\", \" ML100k:\", mls_train.n_items, \"PDA:\", pda_train.n_items)\n",
    "print(\"2) Number of users in each dataset\", \" ML100k:\", mls_train.n_users, \"PDA:\", pda_train.n_users)\n",
    "print(\"3) Number of ratings in each dataset\", \" ML100k:\", mls_train.n_ratings, \"PDA:\", pda_train.n_ratings)\n",
    "print(\"4) Mean rating\", \" ML100k:\", mls_train.global_mean, \"PDA:\", pda_train.global_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprise experiment parameters and variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define auxilliary functions to get Top-N and then calculate precision and recall in Surprise + more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have ratings in the train set for all the users in the test set\n",
    "def find_items_not_in_trainset(trainset, testset):\n",
    "    items_not_in_train = []\n",
    "    for _,itemId, _ in testset:    \n",
    "        if itemId not in trainset.ir.keys():\n",
    "            items_not_in_train.append(itemId)\n",
    "            \n",
    "    return items_not_in_train\n",
    "\n",
    "        \n",
    "def user_seen_items(userId):\n",
    "    return [train_itemId for train_itemId, rating in mls_train.ur[userId]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTopN(predictions, n, minimumRating, criterion):\n",
    "    topN = defaultdict(list)\n",
    "    \n",
    "    for index, row in predictions.iterrows():\n",
    "        if (row[criterion] >= minimumRating):\n",
    "            topN[int(row.uid)].append((int(row.iid), row[criterion]))\n",
    "\n",
    "    for userID, ratings in topN.items():\n",
    "        ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        topN[int(userID)] = ratings[:n]\n",
    "\n",
    "    return topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, items_not_in_train, k, threshold=3.5):\n",
    "    top_n_recoms_est = GetTopN(predictions_df, n=k, minimumRating=threshold, criterion=\"est\")\n",
    "    top_n_recoms_real = GetTopN(predictions_df, n=k, minimumRating=threshold, criterion=\"r_ui\")\n",
    "    above_threshold = predictions_df[predictions_df.r_ui >= threshold]\n",
    "\n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "\n",
    "    for uid, est_topn in top_n_recoms_est.items():\n",
    "        # Get items the user has already rated\n",
    "        already_seen = user_seen_items(uid)\n",
    "        # Get relevant items for the user\n",
    "        n_rel_for_user = len(above_threshold[above_threshold.uid == uid])        \n",
    "        tp = 0\n",
    "        # Penalize the scores if:\n",
    "        # - The item we are recommending was never seen in the training set (how could we recommend what we don't know?)\n",
    "        # - The user has already rated this item: It's not a good recommendation since the user already knows it/has seen it\n",
    "        for est_itemId, _ in est_topn:\n",
    "            if(est_itemId in items_not_in_train or est_itemId in already_seen):\n",
    "                tp += 0\n",
    "            else:\n",
    "                for real_itemId, _ in top_n_recoms_real[uid]:\n",
    "                    if (est_itemId == real_itemId):\n",
    "                        tp +=1\n",
    "        \n",
    "        precisions[uid] = tp/k\n",
    "        recalls[uid] = tp/n_rel_for_user if n_rel_for_user != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(predictions, items_not_in_train, k, threshold=3.5):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" EXPERIMENT VARIABLES\"\"\"\n",
    "# List that will contain the results of all experiments\n",
    "results_table = []\n",
    "kf = KFold(n_splits=5) # define number of k splits for cross validation using Surprise KFold\n",
    "\n",
    "# Algorithms we will be using in this section\n",
    "algorithms = {\n",
    "    \"UserKNN\": KNNWithMeans(k=60, sim_options={'name': 'pearson_baseline', 'shrinkage': 25, 'user_based': True}),\n",
    "    \"ItemKNN\": KNNWithMeans(k=40, sim_options={'name': 'pearson_baseline', 'shrinkage': 25, 'user_based': False}),\n",
    "}\n",
    "# Datasets\n",
    "datasets = {\n",
    "    \"ML100\": movielens_dataset,\n",
    "    \"PDA2018\": pda_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-fold cross validation with UserKNN on ML100 dataset ...\n",
      "Fold  number 0\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 1\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 2\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 3\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 4\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      "Running 5-fold cross validation with ItemKNN on ML100 dataset ...\n",
      "Fold  number 0\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 1\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 2\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 3\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 4\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      "Running 5-fold cross validation with UserKNN on PDA2018 dataset ...\n",
      "Fold  number 0\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 1\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 2\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 3\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 4\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      "Running 5-fold cross validation with ItemKNN on PDA2018 dataset ...\n",
      "Fold  number 0\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 1\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 2\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 3\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 4\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets.keys():\n",
    "    for algorithm in algorithms.keys():\n",
    "        print(\"Running 5-fold cross validation with\", algorithm, \"on\", dataset, \"dataset ...\")\n",
    "        \n",
    "        # Run cross validation\n",
    "        best_prec_5 = 0\n",
    "        best_prec_10 = 0\n",
    "        best_rec_5 = 0\n",
    "        best_rec_10 = 0\n",
    "        fold_nr = 0\n",
    "        \n",
    "        for trainset, testset in kf.split(movielens_dataset):\n",
    "            print(\"Fold  number\", fold_nr)\n",
    "            algorithms[algorithm].fit(trainset)\n",
    "            predictions = algorithms[algorithm].test(testset)\n",
    "            predictions_df = pd.DataFrame(predictions)\n",
    "            predictions_df = predictions_df.iloc[:,:-1]\n",
    "            items_not_in_trainset = find_items_not_in_trainset(trainset, testset)\n",
    "            pres_at_5, recalls_at_5 = precision_recall_at_k(predictions_df, items_not_in_trainset, k=5)\n",
    "            pres_at_10, recalls_at_10 = precision_recall_at_k(predictions_df, items_not_in_trainset, k=10)\n",
    "            avg_pre_5 = np.array([pres_at_5[k] for k in pres_at_5.keys()]).mean()\n",
    "            avg_rec_5 = np.array([recalls_at_5[k] for k in recalls_at_5.keys()]).mean()\n",
    "            avg_pre_10 = np.array([pres_at_10[k] for k in pres_at_10.keys()]).mean()\n",
    "            avg_rec_10 = np.array([recalls_at_10[k] for k in recalls_at_10.keys()]).mean()\n",
    "            if (avg_pre_5 > best_prec_5):\n",
    "                best_prec_5 = avg_pre_5\n",
    "            if (avg_pre_10 > best_prec_10):\n",
    "                best_prec_10 = avg_pre_10\n",
    "            if (avg_rec_5 > best_rec_5):\n",
    "                best_rec_5 = avg_rec_5\n",
    "            if (avg_rec_10 > best_rec_10):\n",
    "                best_rec_10 = avg_rec_10\n",
    "            fold_nr += 1\n",
    "        \n",
    "        new_line = [dataset+\"-\"+algorithm, best_prec_5, best_prec_10, best_rec_5, best_rec_10, 0]\n",
    "        results_table.append(new_line)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Recommender     |    Pre@5 |   Pre@10 |    Rec@5 |   Rec@10 |   NDCG |\n",
      "|:----------------|---------:|---------:|---------:|---------:|-------:|\n",
      "| ML100-UserKNN   | 0.359679 | 0.366204 | 0.32126  | 0.476493 |      0 |\n",
      "| ML100-ItemKNN   | 0.360266 | 0.364151 | 0.326183 | 0.475183 |      0 |\n",
      "| PDA2018-UserKNN | 0.361246 | 0.366858 | 0.323989 | 0.471039 |      0 |\n",
      "| PDA2018-ItemKNN | 0.353111 | 0.358343 | 0.318482 | 0.465402 |      0 |\n"
     ]
    }
   ],
   "source": [
    "# Display results of running the algorithms\n",
    "results_table_headers = [\"Recommender\", \"Pre@5\", \"Pre@10\", \"Rec@5\", \"Rec@10\", \"NDCG\"]\n",
    "print(tabulate(results_table, results_table_headers, tablefmt=\"pipe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Using Cornac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementations of MostPop, BPR and NCF are done by using the Cornac package, which is similiar to Surprise, but contains much more algorithms and metrics. \n",
    "- MostPop is a non-personalized model where the most popular/rated items are recommended to everyone.\n",
    "\n",
    "- NCF (Neural Collaborative Filtering) is a generalization of the matrix factorization problem using a multilayer perceptron.\n",
    "\n",
    "- BPR (Bayesian Personalised Ranking) is esentially a Matrix Factorization algorithm which is optimized with a Bayes Criterion (BPR-OPT) in order to make the recommendation list ranking as personalized to the user (hence as \"correct\") as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cornac\n",
    "from cornac.eval_methods import CrossValidation, RatioSplit\n",
    "from cornac.data import Reader\n",
    "from cornac.data import Dataset\n",
    "from cornac.hyperopt import Discrete, GridSearch\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and load the data using Cornac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens\n",
      "[('196', '242', 3.0), ('186', '302', 3.0), ('22', '377', 1.0), ('244', '51', 2.0), ('166', '346', 1.0)]\n",
      "\n",
      "PDA\n",
      "[('5', '648', 5.0), ('5', '1394', 5.0), ('5', '3534', 5.0), ('5', '104', 4.0), ('5', '2735', 5.0)]\n"
     ]
    }
   ],
   "source": [
    "# Init cornac reader object\n",
    "reader = Reader() # this binarises the data (turns it into implicit feedback)\n",
    "\n",
    "# Read both the datasets from the files using cornac\n",
    "movielens_data = reader.read(fpath=\"../data/u.data\", sep=\"\\t\")\n",
    "pda_data = reader.read(fpath=\"../data/train-PDA2018.csv\", sep=\",\", skip_lines=1)\n",
    "print(\"Movielens\")\n",
    "print(movielens_data[:5])\n",
    "print()\n",
    "print(\"PDA\")\n",
    "print(pda_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Popular model\n",
    "most_pop_model = cornac.models.MostPop()\n",
    "\n",
    "# NCF model: We'll use the Pre-Trained NeuMF model, since it performs better as shown in https://arxiv.org/pdf/1708.05031.pdf\n",
    "gmf = cornac.models.GMF(\n",
    "    num_factors=8,\n",
    "    num_epochs=10,\n",
    "    learner=\"adam\",\n",
    "    batch_size=256,\n",
    "    lr=0.001,\n",
    "    num_neg=50,\n",
    "    seed=123,\n",
    ")\n",
    "mlp = cornac.models.MLP(\n",
    "    layers=[64, 32, 16, 8],\n",
    "    act_fn=\"tanh\",\n",
    "    learner=\"adam\",\n",
    "    num_epochs=5,\n",
    "    batch_size=256,\n",
    "    lr=0.001,\n",
    "    num_neg=50,\n",
    "    seed=123,\n",
    ")\n",
    "ncf_model = cornac.models.NeuMF(\n",
    "    name=\"NeuMF_pretrained\",\n",
    "    learner=\"adam\",\n",
    "    num_epochs=5,\n",
    "    batch_size=256,\n",
    "    lr=0.001,\n",
    "    num_neg=50,\n",
    "    seed=123,\n",
    "    num_factors=gmf.num_factors,\n",
    "    layers=mlp.layers,\n",
    "    act_fn=mlp.act_fn,\n",
    ").pretrain(gmf, mlp)\n",
    "\n",
    "# BPR Model\n",
    "bpr_model = cornac.models.BPR(\n",
    "    k=10,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.01\n",
    ")\n",
    "\n",
    "# Eval Metrics\n",
    "pre_5 = cornac.metrics.Precision(k=5)\n",
    "pre_10 = cornac.metrics.Precision(k=10)\n",
    "rec_5 = cornac.metrics.Recall(k=5)\n",
    "rec_10 = cornac.metrics.Recall(k=10)\n",
    "ndcg = cornac.metrics.NDCG()\n",
    "auc = cornac.metrics.AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" EXPERIMENT VARIABLES\"\"\"\n",
    "# List that will contain the RMSE and MAE results\n",
    "cv_n_folds = 5 # define number of k splits for cross validation\n",
    "rating_threshold = 3 # This parameter is the threshold used for ranking metrics\n",
    "\n",
    "# Algorithms we will be using in this section\n",
    "cornac_algorithms = {\n",
    "    \"MostPop\": most_pop_model,\n",
    "    \"BPR\": bpr_model, \n",
    "    \"NCF\": ncf_model\n",
    "}\n",
    "# Datasets\n",
    "datasets = {\n",
    "    \"ML100\": movielens_data,\n",
    "    \"PDA2018\": pda_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-fold cross validation with MostPop on ML100 dataset ...\n",
      "\n",
      "\n",
      "rating_threshold = 3.0\n",
      "exclude_unknowns = True\n",
      "Fold: 1\n",
      "---\n",
      "Training data:\n",
      "Number of users = 943\n",
      "Number of items = 1648\n",
      "Number of ratings = 80000\n",
      "Max rating = 5.0\n",
      "Min rating = 1.0\n",
      "Global mean = 3.5\n",
      "---\n",
      "Test data:\n",
      "Number of users = 943\n",
      "Number of items = 1382\n",
      "Number of ratings = 19966\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Validation data:\n",
      "Number of users = 943\n",
      "Number of items = 1382\n",
      "Number of ratings = 19966\n",
      "---\n",
      "Total users = 943\n",
      "Total items = 1648\n",
      "\n",
      "[MostPop] Training started!\n",
      "\n",
      "[MostPop] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e567dcc19db44749471f88eaa5e0a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Ranking', max=943.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold: 2\n",
      "---\n",
      "Training data:\n",
      "Number of users = 943\n",
      "Number of items = 1652\n",
      "Number of ratings = 80000\n",
      "Max rating = 5.0\n",
      "Min rating = 1.0\n",
      "Global mean = 3.5\n",
      "---\n",
      "Test data:\n",
      "Number of users = 942\n",
      "Number of items = 1371\n",
      "Number of ratings = 19967\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Validation data:\n",
      "Number of users = 942\n",
      "Number of items = 1371\n",
      "Number of ratings = 19967\n",
      "---\n",
      "Total users = 943\n",
      "Total items = 1652\n",
      "\n",
      "[MostPop] Training started!\n",
      "\n",
      "[MostPop] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239e6505a54d4bc99d988fe35566b2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Ranking', max=942.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold: 3\n",
      "---\n",
      "Training data:\n",
      "Number of users = 943\n",
      "Number of items = 1651\n",
      "Number of ratings = 80000\n",
      "Max rating = 5.0\n",
      "Min rating = 1.0\n",
      "Global mean = 3.5\n",
      "---\n",
      "Test data:\n",
      "Number of users = 940\n",
      "Number of items = 1390\n",
      "Number of ratings = 19965\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Validation data:\n",
      "Number of users = 940\n",
      "Number of items = 1390\n",
      "Number of ratings = 19965\n",
      "---\n",
      "Total users = 943\n",
      "Total items = 1651\n",
      "\n",
      "[MostPop] Training started!\n",
      "\n",
      "[MostPop] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6d395e62ad455a8d57fc3916e3307d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Ranking', max=940.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold: 4\n",
      "---\n",
      "Training data:\n",
      "Number of users = 943\n",
      "Number of items = 1656\n",
      "Number of ratings = 80000\n",
      "Max rating = 5.0\n",
      "Min rating = 1.0\n",
      "Global mean = 3.5\n",
      "---\n",
      "Test data:\n",
      "Number of users = 943\n",
      "Number of items = 1397\n",
      "Number of ratings = 19969\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Validation data:\n",
      "Number of users = 943\n",
      "Number of items = 1397\n",
      "Number of ratings = 19969\n",
      "---\n",
      "Total users = 943\n",
      "Total items = 1656\n",
      "\n",
      "[MostPop] Training started!\n",
      "\n",
      "[MostPop] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623b296752ba47409e42ad9b7f3be978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Ranking', max=943.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold: 5\n",
      "---\n",
      "Training data:\n",
      "Number of users = 943\n",
      "Number of items = 1646\n",
      "Number of ratings = 80000\n",
      "Max rating = 5.0\n",
      "Min rating = 1.0\n",
      "Global mean = 3.5\n",
      "---\n",
      "Test data:\n",
      "Number of users = 942\n",
      "Number of items = 1383\n",
      "Number of ratings = 19959\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Validation data:\n",
      "Number of users = 942\n",
      "Number of items = 1383\n",
      "Number of ratings = 19959\n",
      "---\n",
      "Total users = 943\n",
      "Total items = 1646\n",
      "\n",
      "[MostPop] Training started!\n",
      "\n",
      "[MostPop] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85886fb3b5c499eb97550d2f37d07e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Ranking', max=942.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TEST:\n",
      "...\n",
      "[MostPop]\n",
      "       |    AUC | NDCG@-1 | Precision@10 | Precision@5 | Recall@10 | Recall@5 | Train (s) | Test (s)\n",
      "------ + ------ + ------- + ------------ + ----------- + --------- + -------- + --------- + --------\n",
      "Fold 0 | 0.8624 |  0.4056 |       0.0886 |      0.0946 |    0.0799 |   0.0431 |    0.0073 |   1.4622\n",
      "Fold 1 | 0.8660 |  0.4058 |       0.0888 |      0.0939 |    0.0785 |   0.0426 |    0.0065 |   1.2308\n",
      "Fold 2 | 0.8622 |  0.4000 |       0.0887 |      0.0898 |    0.0848 |   0.0397 |    0.0061 |   1.1860\n",
      "Fold 3 | 0.8630 |  0.4054 |       0.0905 |      0.1020 |    0.0867 |   0.0472 |    0.0069 |   1.2611\n",
      "Fold 4 | 0.8617 |  0.4050 |       0.0930 |      0.1036 |    0.0873 |   0.0466 |    0.0062 |   1.1938\n",
      "------ + ------ + ------- + ------------ + ----------- + --------- + -------- + --------- + --------\n",
      "Mean   | 0.8631 |  0.4044 |       0.0899 |      0.0968 |    0.0834 |   0.0438 |    0.0066 |   1.2668\n",
      "Std    | 0.0015 |  0.0022 |       0.0017 |      0.0052 |    0.0036 |   0.0028 |    0.0005 |   0.1014\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'NDCG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-fe3a78944e51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mresults_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_avg_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             new_line = [dataset+\"-\"+algorithm, results_dict['Precision@5'], results_dict['Precision@10'], \\\n\u001b[0;32m---> 22\u001b[0;31m                         results_dict['Recall@5'], results_dict['Recall@5'], results_dict['NDCG']]\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mresults_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'NDCG'"
     ]
    }
   ],
   "source": [
    "for dataset in datasets.keys():\n",
    "    for algorithm in cornac_algorithms.keys():\n",
    "        print(\"Running 5-fold cross validation with\", algorithm, \"on\", dataset, \"dataset ...\\n\\n\")\n",
    "        # Define Cornac cross validation object\n",
    "        cv = CrossValidation(\n",
    "            data=datasets[dataset],\n",
    "            n_folds=cv_n_folds,\n",
    "            rating_threshold=rating_threshold, # This parameter is the threshold used for ranking metrics\n",
    "            seed = 0,\n",
    "            verbose=True\n",
    "        )\n",
    "        # Define Cornac experiment (put everything together)\n",
    "        experiment = cornac.Experiment(\n",
    "            eval_method=cv,\n",
    "            models=[cornac_algorithms[algorithm]],    \n",
    "            metrics=[pre_5, pre_10, rec_5, rec_10, auc, ndcg],\n",
    "        )\n",
    "        experiment.run()\n",
    "        for entry in experiment.result:\n",
    "            results_dict = entry[0].metric_avg_results\n",
    "            new_line = [dataset+\"-\"+algorithm, results_dict['Precision@5'], results_dict['Precision@10'], \\\n",
    "                        results_dict['Recall@5'], results_dict['Recall@5'], results_dict['NDCG@-1']]\n",
    "            results_table.append(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Recommender     |    Pre@5 |   Pre@10 |    Rec@5 |   Rec@10 |\n",
      "|:----------------|---------:|---------:|---------:|---------:|\n",
      "| ML100-UserKNN   | 0.357078 | 0.360046 | 0.316812 | 0.4689   |\n",
      "| ML100-ItemKNN   | 0.358567 | 0.360022 | 0.312494 | 0.465268 |\n",
      "| PDA2018-UserKNN | 0.354259 | 0.360185 | 0.316559 | 0.46891  |\n",
      "| PDA2018-ItemKNN | 0.356278 | 0.354036 | 0.32095  | 0.468115 |\n"
     ]
    }
   ],
   "source": [
    "# Display results of running the algorithms\n",
    "results_table_headers = [\"Recommender\", \"Pre@5\", \"Pre@10\", \"Rec@5\", \"Rec@10\", \"NDCG\"]\n",
    "print(tabulate(results_table, results_table_headers, tablefmt=\"pipe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "# Export the results to a csv file\n",
    "results_df = pd.DataFrame(results_table, columns=[\"Recommender\", \"Pre@5\", \"Pre@10\", \"Rec@5\", \"Rec@10\", \"NDCG\"])\n",
    "results_df.to_csv(\"../data/rating_prediction_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
